# üß† LM-Local-Evaluation

Mithilfe dieses Repositories k√∂nnen Language Models wie **Qwen**, **Gemma** oder **LLaMA 3** evaluiert sowie untereinander verglichen werden.

Dazu werden je Modell folgende Kategorien gemessen, ausgewertet und grafisch dargestellt:
- Installationsgr√∂√üe
- Latenz
- RAM-Speichernutzung
- GPU-Speichernutzung
- Parametergr√∂√üe
- Antwortqualit√§t

Dies dient dem Vergleich von verschiedenen Language Models zur lokalen Nutzung als pers√∂nlichen Assistenten.
Dadurch kann herausgefunden werden, ob der Einsatz eines bestimmten Language Models f√ºr eine bestimmte Hardware-Konfiguration, beispielsweise ein mobiles Endger√§ts, sinnvoll ist.
Je nach Benutzer-Pr√§ferenz k√∂nnen hierbei andere Schwerpunkte gesetzt werden und entsprechend eine Auswahl getroffen werden. Dabei soll dieses Repository unterst√ºtzen.

Dieses Repository enth√§lt eine lokal integrierte und angepasste Version von [EleutherAI/lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness) f√ºr die Ermittlung der Antwortqualit√§t, daher muss kein separates GitHub-Repo geklont werden. Alle √Ñnderungen sind enthalten und einsatzbereit.

---

## üîß Voraussetzungen

- Python 3.11
- Git
- Windows 11 oder Linux
- NVIDIA GPU mit CUDA 12.x (z.B. GeForce RTX 3060)
- NVIDIA CUDA-Toolkit + cuDNN-Bibliothek
- Huggingface Account & Access Token
- Virtual Environment (empfohlen)

---

## ü™ü Anleitung Installation (neues Windows 11 System)

1. Python 3.11.0 herunterladen und installieren
    - Python 3.11.0 von der [offiziellen Webseite](https://www.python.org/downloads/release/python-3110/) herunterladen und installieren
    - mit "python --info" in der Konsole pr√ºfen, ob die Installation geklappt hat
2. Git herunterladen und installieren
    - Git von der [offiziellen Webseite](https://git-scm.com/downloads/win) herunterladen und installieren
    - mit "git --version" in der Konsole pr√ºfen, ob die Installation geklappt hat
3. Git Umgebungsvariable setzen
    - Windows+S Taste dr√ºcken: ‚ÄúUmgebungsvariable bearbeiten‚Äù suchen
    - W√§hle ‚ÄúUmgebungsvariablen‚Äù
    - unter ‚ÄúSystemvariablen‚Äù, zu ‚ÄúPath‚Äù gehen und ‚ÄúBearbeiten‚Äù ausw√§hlen
    - ‚ÄúNeu‚Äù anklicken und den Installationspfad von Git hinzuf√ºgen (z.B. C:\Program Files\Git\bin)
    - auf ‚ÄúOK‚Äù klicken und alle Fenster schlie√üen
    - Terminal neu starten und mit ‚Äúgit --version‚Äù testen, ob git erkannt wird
4. CUDA installieren
    - √ºberpr√ºfen, ob die eigene [Nvidia GPU CUDA f√§hig](https://developer.nvidia.com/cuda-gpus) ist
    - neueste Nvidia Grafikkarten-Softwareversion √ºber z.B. Geforce Experience installieren
    - CUDA Toolkit f√ºr Windows [herunterladen](https://developer.nvidia.com/cuda-downloads) und installieren
    - Computer neustarten
5. cuDNN-Bibliothek installieren
    - cuDNN-Bibliothek [herunterladen](https://developer.nvidia.com/cudnn-downloads) (z.B. Windows - x86_64 - Tarball - 12)
    - heruntergeladene Dateien entpacken
    - einzelne Dateien (bin, include, lib) in die Verzeichnisse des CUDA-Installationsordners kopieren (z.B. Default: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.8)
6. CUDA Umgebungsvariable setzen
    - Windows+S Taste dr√ºcken: ‚ÄúUmgebungsvariable bearbeiten‚Äù suchen
    - W√§hle ‚ÄúUmgebungsvariablen‚Äù
    - unter ‚ÄúSystemvariablen‚Äù, zu ‚ÄúPath‚Äù gehen und ‚ÄúBearbeiten‚Äù ausw√§hlen
    - ‚ÄúNeu‚Äù anklicken und den Installationspfad von CUDA hinzuf√ºgen (z.B. C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.8\bin\)
    - auf ‚ÄúOK‚Äù klicken und alle Fenster schlie√üen
    - Terminal neu starten und mit ‚Äúnvcc --version‚Äù testen, ob CUDA erkannt wird
7. Virtual Environment anlegen und starten
    - Order erstellen, in welchem die Scripte und Modelle gespeichert werden sollen (z.B. E:\Daten\LM_Benchmarks)
    - im Windows-Terminal in den neuen Ordner wechseln (z.B. ‚Äúcd /d E:\Daten\LM_Benchmarks‚Äù)
    - Virtuelle Umgebung erstellen (z.B. mit ‚Äúpy -3.11 -m venv venv‚Äù)
    - Windows Ausf√ºhrungsrechte f√ºr den aktuellen Benutzer setzen (z.B. √ºber PowerShell als Administrator: Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser)
    - Virtuelle Umgebung starten (z.B. mit ‚Äú./venv/Scripts/activate‚Äù)
8. lm-local-evaluation Repository klonen
    - Repository klonen mit: ‚Äúgit clone https://github.com/NiklasS1999/lm-local-evaluation.git‚Äù
    - Repository √∂ffnen mit: ‚Äúcd lm-local-evaluation‚Äù
9. Abh√§ngigkeiten installieren
    - alle ben√∂tigten Abh√§ngigkeiten mit ‚Äúpip install -r requirements.txt‚Äú installieren
10. Huggingface Login und Konfiguration
    - Account unter [Huggingface anlegen](https://huggingface.co/join)
    - einen Access-Token [unter den Accounteinstellungen](https://huggingface.co/settings/tokens) anlegen und kopieren
    - in der Windows Konsole ‚Äúhuggingface-cli login‚Äù eingeben
    - Token versteckt (man sieht die Eingabe nicht!) mit einem Rechtsklick einf√ºgen und Enter dr√ºcken
    - bei der Abfrage Y eingeben und Enter dr√ºcken
    - Huggingface Model-Download Ordner setzen (z.B. √ºber die Konsole mit ‚Äúsetx HF_HOME "E:\Daten\LM_Benchmarks\models_cache"‚Äù)


## üêß Anleitung Installation (neues Windows 11 WSL2 Linux System)

1. Im BIOS des Mainboards den SVM Mode aktivieren (damit eine Virtualisierung m√∂glich ist)
2. WSL2 (Ubuntu) installieren und konfigurieren
    - in der Konsole folgendes eingeben: ‚Äúwsl --install -d Ubuntu‚Äù
    - in der Konsole folgendes eingeben: ‚Äúwsl.exe --update‚Äù
    - Computer neustarten
    - Windows+S Taste dr√ºcken: ‚ÄúUbuntu‚Äù suchen und √∂ffnen
    - Benutzernamen und Passwort setzen
    - Linux Version aktualisieren (z.B. mit ‚Äúsudo apt update && sudo apt upgrade -y‚Äù)
    - Paketinstaller aktualisieren (z.B. mit ‚Äúsudo apt update‚Äù)
    - Python 3.11, Venv, Distutils, Git, Essentials, Nano installieren
        - ‚Äúsudo apt install -y software-properties-common‚Äù
        - ‚Äúsudo add-apt-repository ppa:deadsnakes/ppa‚Äù
        - ‚Äúsudo apt install -y python3.11 python3.11-venv python3.11-distutils python3.11-dev git build-essential nano‚Äù
3. CUDA installieren
    - wie in der Windows Anleitung unter Punkt 4. beschrieben vorgehen (f√ºr WSL2 Ubuntu)
    - CUDA-Toolkit direkt unter Ubuntu installieren (z.B. mit ‚Äúsudo apt install -y nvidia-cuda-toolkit‚Äù)
4. Virtual Environment anlegen und starten
    - Order erstellen, in welchem die Scripte und Modelle gespeichert werden sollen
    - im Ubuntu-Terminal in den neuen Ordner wechseln (z.B. ‚Äúcd ./Daten/LM_Benchmarks‚Äù)
    - VENV anlegen mit: ‚Äúpython3.11 -m venv venv‚Äù
    - VENV aktivieren mit: ‚Äúsource venv/bin/activate‚Äù
5. lm-local-evaluation Repository klonen
    - Repository klonen mit: ‚Äúgit clone https://github.com/NiklasS1999/lm-local-evaluation.git‚Äù
    - Repository √∂ffnen mit: ‚Äúcd lm-local-evaluation‚Äù
6. Abh√§ngigkeiten installieren
    - Pip aktualisieren mit ‚Äúpip install --upgrade pip‚Äù
    - alle ben√∂tigten Abh√§ngigkeiten mit ‚Äúpip install -r requirements.txt‚Äú installieren
7. Huggingface Login und Konfiguration
    - Account unter [Huggingface anlegen](https://huggingface.co/join)
    - einen Access-Token [unter den Accounteinstellungen](https://huggingface.co/settings/tokens) anlegen und kopieren
    - in der Ubuntu Konsole ‚Äúhuggingface-cli login‚Äù eingeben
    - Token versteckt (man sieht die Eingabe nicht!) mit einem Rechtsklick einf√ºgen und Enter dr√ºcken
    - bei der Abfrage Y eingeben und Enter dr√ºcken

---

## ‚öôÔ∏è Konfiguration

Die Benchmarks k√∂nnen mithilfe von verschiedenen Einstellungen ausgef√ºhrt werden. Diese wirken sich drastisch auf die Ergebnisqualit√§t sowie die Ausf√ºhrungszeit aus.
Die Konfiguration kann mithilfe der Datei global_config.py angepasst werden, diese ist ausf√ºhrlich kommentiert und zeigt m√∂gliche Anpassungen auf.

Bearbeiten der Konfiguration:
```bash
nano global_config.py
```

---

## üöÄ Ausf√ºhrung

Es k√∂nnen entweder einzelne Benchmarks oder alle automatisiert nacheinander ausgef√ºhrt werden.

Liste an vorhandenen Benchmarks:
model_installationsize.py   -> Messung der Installationsgr√∂√üe der einzelnen Modelle
model_latency.py            -> Messung der Latenz (Model- und Tokenizer-Laden Latenz, Antwortlatenz, Gesamt)
model_memoryusage.py        -> Messung der RAM-Speichernutzung (Cold-Start, nur Inferenz, GPU-Speicher)
model_parametersize.py      -> Messung der Parametergr√∂√üe und somit des groben (vermutlichen) Rechenaufwands
model_responsequality.py    -> Messung der Antwortqualit√§t durch vordefinierte Benchmarks des [EleutherAI/lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness) Repository wie MMLU, HellaSwag, GSM8K, HumanEval, BoolQ

Ausf√ºhrung eines einzelnen Benchmarks:
```bash
python model_installationsize.py
```
Ausf√ºhrung aller Benchmarks inkl. Auswertung:
```bash
python run_all_benchmarks.py
```

---

## üìä Ergebnisse

W√§hrend der Ausf√ºhrung eines Benchmarks werden die Ergebnisse in der Konsole ausgegeben. Au√üerdem werden die Ergebnisse am Ende des Benchmarks entweder in eine .csv oder eine .json Datei mit dem Namen des Benchmarks sowie einem Zeitstempel geschrieben und unter ./results/Modell_Name abgespeichert.

Wenn alle Benchmarks mithilfe von run_all_benchmarks.py ausgef√ºhrt werden, wird am Ende eine Auswertung durchgef√ºhrt.
Durch diese wird eine √úbersicht der wichtigsten Ergebnisse aller durchgef√ºhrten Benchmarks in der Konsole ausgegeben, sowie die wichtigsten Ergebnisse unter ./results/benchmark_overview.csv abgespeichert.

Au√üerdem werden im Rahmen der Auswertung verschiedene Plots (Balkendiagramme) zum Vergleich der evaluierten Language Models generiert und unter ./results/result_plots abgespeichert.

Des Weiteren wird eine Auswertung bez√ºglich der jeweiligen Vor- und Nachteile eines Modells in Bezug auf den Vergleich zu den anderen Modellen in der Konsole ausgegeben. Diese Ergebnisse werden au√üerdem in einer Textdatei unter ./results/model_advantages.txt abgespeichert.

Die Auswertung kann auch manuell √ºber den folgenden Befehlt erstellt werden:
```bash
python benchmark_overview.py
```
Es werden dann jedoch nur die Ergebnisse dargestellt, welche auch vorhanden sind (wo bisher Benchamarks durchgef√ºhrt wurden).

---

## üí° Tipps

Manche Antwortqualit√§t-Benchmarks sind nur unter einem Linux-Betriebssystem ausf√ºhrbar und werfen unter der Ausf√ºhrung in Windows einen Fehler (z.B. HumanEval).

Da das Projekt vollst√§ndig mit Python umgesetzt wurde, ist der Code betriebssystemunabh√§ngig, jedoch wurde der Code nur unter Windows 11 und WSL2-Linux (Ubuntu) getestet.

Die Ausf√ºhrungszeit ist je nach Konfiguration sehr unterscheidlich, in der von mir bereitgestellten Konfiguration dauerte die vollst√§ndige Auswertung insgesamt 19 Stunden mit einer GeForce RTX 3060.
Dies ist vorallem auf den Antwortqualit√§t-Benchmark HumanEval zur√ºckzuf√ºhren. Dieser dauerte alleine rund 15 Stunden f√ºr alle drei Language Modelle.

Je nach Language Modell kann es sein, das in der Konsole Fehler auftreten. Besonders bei den Antwortqualit√§t-Benchmarks kann dies aufgrund von Inkompatibilit√§t vorkommen.
Getestet wurden lediglich die drei in der Konfiguration hinterlegten Modelle.

Wenn ein anderes Modell getestet werden soll, muss in der Konfiguration unter "models": der Huggignface-pFad zum Language Model angegeben werden.
Dementsprechend k√∂nnen nur Language Models gebenchmarkt werden, die auch in Huggignface √∂ffentlich zug√§nglich sind.

Bei der ersten Ausf√ºhrung eines Benchmarks werden die entsprechend in der Globalen konfiguration hinterlegten Modelle von Huggingface heruntergeladen.
Dies kann je nach Modellgr√∂√üe und Internetverbindung mehrere Stunden in Anspruch nehmen.


