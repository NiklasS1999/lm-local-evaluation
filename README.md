# üß† LM-Local-Evaluation

Mithilfe dieses Repositories k√∂nnen Language Models wie **Qwen**, **Gemma** oder **LLaMA 3** evaluiert sowie untereinander verglichen werden.

Dazu werden je Modell folgende Kategorien gemessen, ausgewertet und grafisch dargestellt:
- Installationsgr√∂√üe
- Parametergr√∂√üe
- Latenz
- RAM-Speichernutzung
- GPU-Speichernutzung
- Antwortqualit√§t

Dies dient dem Vergleich von verschiedenen Language Models zur lokalen Nutzung als pers√∂nlichen Assistenten.<br>
Dadurch kann herausgefunden werden, ob der Einsatz eines bestimmten Language Models f√ºr eine bestimmte Hardware-Konfiguration, beispielsweise ein mobiles Endger√§ts, sinnvoll ist.<br>
Je nach Benutzer-Pr√§ferenz k√∂nnen hierbei andere Schwerpunkte gesetzt werden und entsprechend eine Auswahl getroffen werden.<br>

Dieses Repository enth√§lt eine lokal integrierte und angepasste Version von [EleutherAI/lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness) f√ºr die Ermittlung der Antwortqualit√§t, daher muss kein separates GitHub-Repo geklont werden. Alle √Ñnderungen sind enthalten und einsatzbereit.

---

## üîß Voraussetzungen

- Python 3.11
- Git
- Windows 11 oder Linux
- NVIDIA GPU mit CUDA 12.x (z.B. GeForce RTX 3060)
- NVIDIA CUDA-Toolkit + cuDNN-Bibliothek
- Huggingface Account & Access Token
- Virtual Environment (empfohlen)

---

## ü™ü Anleitung Installation (neues Windows 11 System)

1. **Python 3.11.0 herunterladen und installieren**
    - Python 3.11.0 von der [offiziellen Webseite](https://www.python.org/downloads/release/python-3110/) herunterladen und installieren
    - √úberpr√ºfen, ob die Installation geklappt hat:
    ```bash
    python --version
    ```
2. **Git herunterladen und installieren**
    - Git von der [offiziellen Webseite](https://git-scm.com/downloads/win) herunterladen und installieren
    - √úberpr√ºfen, ob die Installation geklappt hat
    ```bash
    git --version
    ```
3. **Git Umgebungsvariable setzen**
    - Windows+S Taste dr√ºcken: ‚ÄúUmgebungsvariable bearbeiten‚Äù suchen
    - W√§hle ‚ÄúUmgebungsvariablen‚Äù
    - Unter ‚ÄúSystemvariablen‚Äù, zu ‚ÄúPath‚Äù gehen und ‚ÄúBearbeiten‚Äù ausw√§hlen
    - ‚ÄúNeu‚Äù anklicken und den Installationspfad von Git hinzuf√ºgen (z.B. C:\Program Files\Git\bin)
    - Auf ‚ÄúOK‚Äù klicken und alle Fenster schlie√üen
    - Terminal neu starten und testen, ob git erkannt wird
    ```bash
    git --version
    ```
4. **CUDA installieren**
    - √úberpr√ºfen, ob die eigene [Nvidia GPU CUDA f√§hig](https://developer.nvidia.com/cuda-gpus) ist
    - Neueste Nvidia Grafikkarten-Softwareversion √ºber z.B. Geforce Experience installieren
    - CUDA Toolkit f√ºr Windows [herunterladen](https://developer.nvidia.com/cuda-downloads) und installieren
    - Computer neustarten
5. **cuDNN-Bibliothek installieren**
    - cuDNN-Bibliothek [herunterladen](https://developer.nvidia.com/cudnn-downloads) (z.B. Windows - x86_64 - Tarball - 12)
    - Heruntergeladene Dateien entpacken
    - Einzelne Dateien (bin, include, lib) in die Verzeichnisse des CUDA-Installationsordners kopieren (z.B. Default: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.8)
6. **CUDA Umgebungsvariable setzen**
    - Windows+S Taste dr√ºcken: ‚ÄúUmgebungsvariable bearbeiten‚Äù suchen
    - W√§hle ‚ÄúUmgebungsvariablen‚Äù
    - Unter ‚ÄúSystemvariablen‚Äù, zu ‚ÄúPath‚Äù gehen und ‚ÄúBearbeiten‚Äù ausw√§hlen
    - ‚ÄúNeu‚Äù anklicken und den Installationspfad von CUDA hinzuf√ºgen (z.B. C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.8\bin\)
    - Auf ‚ÄúOK‚Äù klicken und alle Fenster schlie√üen
    - Terminal neu starten und testen, ob CUDA erkannt wird
    ```bash
    nvcc --version
    ```
7. **lm-local-evaluation Repository klonen**
    - Order erstellen, in welchem die Scripte und Modelle gespeichert werden sollen
    ```bash
    mkdir LM_Benchmarks
    ```

    - In den neuen Ordner wechseln
    ```bash
    cd .\LM_Benchmarks
    ```

    - GitHub-Repository klonen
    ```bash
    git clone https://github.com/NiklasS1999/lm-local-evaluation.git
    ```

    - GitHub-Repository √∂ffnen
    ```bash
    cd lm-local-evaluation
    ```
8. **Virtual Environment anlegen und starten**
    - Virtuelle Umgebung erstellen
    ```bash
    python -m venv venv
    ```

    - Windows Ausf√ºhrungsrechte f√ºr den aktuellen Benutzer setzen (√ºber PowerShell als Administrator)
    ```bash
    Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser
    ```

    - Virtuelle Umgebung starten
    ```bash
    .\venv\Scripts\activate
    ```
9. **Modulabh√§ngigkeiten installieren**
    - Pip aktualisieren
    ```bash
    python.exe -m pip install --upgrade pip
    ```

    - Ben√∂tigte Abh√§ngigkeiten installieren
    ```bash
    pip install -r requirements.txt
    ```
10. **Huggingface Login und Konfiguration**
    - Account unter [Huggingface anlegen](https://huggingface.co/join)
    - Access-Token [unter den Accounteinstellungen](https://huggingface.co/settings/tokens) anlegen und kopieren
    - √úber Windows-Konsole einloggen
    ```bash
    huggingface-cli login
    ```

    - Token versteckt (man sieht die Eingabe nicht!) mit einem Rechtsklick einf√ºgen und Enter dr√ºcken
    - Bei der R√ºckfrage Y eingeben und Enter dr√ºcken
    - Huggingface Model-Download Ordner setzen
    ```bash
    setx HF_HOME "E:\Daten\LM_Benchmarks\models_cache"
    ```


## üêß Anleitung Installation (neues Windows 11 WSL2 Linux System)

1. **SVM-Mode aktivieren**
    - Computer neustarten
    - BIOS-Zugriffstaste w√§hrend des Bootvorgangs klicken (meist F1 / F2 / Entf)
    - Im BIOS den SVM-Mode aktivieren (damit eine Virtualisierung m√∂glich ist)
2. **WSL2 installieren und konfigurieren**
    - WSL (Ubuntu) installieren
    ```bash
    wsl --install -d Ubuntu
    ```

    - WSL (Ubuntu) updaten
    ```bash
    wsl.exe --update
    ```

    - Computer neustarten
    - Windows+S Taste dr√ºcken: ‚ÄúUbuntu‚Äù suchen und √∂ffnen
    - Benutzernamen und Passwort setzen
    - Linux Version aktualisieren
    ```bash
    sudo apt update && sudo apt upgrade -y
    ```

    - Paketinstaller aktualisieren
    ```bash
    sudo apt update
    ```

    - Python 3.11, Venv, Distutils, Git, Essentials, Nano installieren
    ```bash
    sudo apt install -y software-properties-common
    sudo add-apt-repository ppa:deadsnakes/ppa
    sudo apt install -y python3.11 python3.11-venv python3.11-distutils python3.11-dev git build-essential nano
    ```
3. **CUDA installieren**
    - Wie in der oberen Windows Anleitung unter Punkt 4. beschrieben vorgehen (f√ºr WSL2 Ubuntu)
    - CUDA-Toolkit direkt unter Ubuntu installieren
    ```bash
    sudo apt install -y nvidia-cuda-toolkit
    ```
4. **lm-local-evaluation Repository klonen**
    - Order erstellen, in welchem die Scripte und Modelle gespeichert werden sollen
    ```bash
    mkdir LM_Benchmarks
    ```

    - In den neuen Ordner wechseln
    ```bash
    cd ./LM_Benchmarks
    ```

    - GitHub-Repository klonen
    ```bash
    git clone https://github.com/NiklasS1999/lm-local-evaluation.git
    ```

    - GitHub-Repository √∂ffnen
    ```bash
    cd lm-local-evaluation
    ```
5. **Virtual Environment anlegen und starten**
    - Virtuelle Umgebung erstellen
    ```bash
    python3.11 -m venv venv
    ```

    - Virtuelle Umgebung starten
    ```bash
    source venv/bin/activate
    ```
6. **Abh√§ngigkeiten installieren**
    - Pip aktualisieren
    ```bash
    pip install --upgrade pip
    ```

    - Ben√∂tigte Abh√§ngigkeiten installieren
    ```bash
    pip install -r requirements.txt
    ```
7. **Huggingface Login und Konfiguration**
    - Account unter [Huggingface anlegen](https://huggingface.co/join)
    - Access-Token [unter den Accounteinstellungen](https://huggingface.co/settings/tokens) anlegen und kopieren
    - √úber Ubuntu-Konsole einloggen
    ```bash
    huggingface-cli login
    ```

    - Token versteckt (man sieht die Eingabe nicht!) mit einem Rechtsklick einf√ºgen und Enter dr√ºcken
    - Bei der Abfrage Y eingeben und Enter dr√ºcken

---

## ‚öôÔ∏è Konfiguration

Die Benchmarks k√∂nnen mithilfe von verschiedenen Einstellungen ausgef√ºhrt werden. Diese wirken sich drastisch auf die Ergebnisqualit√§t sowie die Ausf√ºhrungszeit aus.<br>
Die Konfiguration kann mithilfe der Datei global_config.py angepasst werden, diese ist ausf√ºhrlich kommentiert und zeigt m√∂gliche Anpassungen auf.<br>

Neben der globalen Konfiguration kann auch je Benchmark eine eigene Konfiguration gesetzt werden.<br>
Dies geschieht in den einzelnen Benchmark-Scripten.<br>

Bearbeiten der Globalen Konfiguration:
```bash
cd ./benchmarks # in den Benchmark-Ordner wechseln, wenn noch nicht getan
nano global_config.py # nur unter Linux
notepad global_config.py # nur unter Windows
```

Bearbeiten einer lokalen Konfiguration:
```bash
cd ./benchmarks # in den Benchmark-Ordner wechseln, wenn noch nicht getan
nano model_latency.py # nur unter Linux
notepad model_latency.py # nur unter Windows
```

---

## üöÄ Ausf√ºhrung

Es k√∂nnen entweder einzelne Benchmarks oder alle automatisiert nacheinander ausgef√ºhrt werden.

Liste an vorhandenen Benchmarks:

| Benchmark-Skript          | Beschreibung                                                                                      |
|------------------------------|-------------------------------------------------------------------------------------------------------|
| `model_installationsize.py`  | Misst die **Installationsgr√∂√üe** des Modells.                                                        |
| `model_parametersize.py`     | Misst die **Gr√∂√üe der Modellparameter** ‚Äì als Anhaltspunkt f√ºr den gesch√§tzten Rechenaufwand.        |
| `model_latency.py`           | Misst die **Latenz**: Tokenizer-, Modell-, Antwort- und Gesamtlatenz.                                |
| `model_memoryusage.py`       | Misst die **RAM- und GPU-Speichernutzung**: Cold-Start, reine Inferenz, Gesamtverbrauch.             |
| `model_responsequality.py`   | Bewertet die **Antwortqualit√§t** anhand vordefinierter Benchmarks (z.‚ÄØB. MMLU, HellaSwag, GSM8K).    |

Ausf√ºhrung eines einzelnen Benchmarks:
```bash
cd ./benchmarks # in den Benchmark-Ordner wechseln, wenn noch nicht getan
python model_installationsize.py
```

Ausf√ºhrung aller Benchmarks inkl. Auswertung:
```bash
cd ./benchmarks # in den Benchmark-Ordner wechseln, wenn noch nicht getan
python run_all_benchmarks.py
```

---

## üìä Ergebnisse

W√§hrend der Ausf√ºhrung eines Benchmarks werden die Ergebnisse in der Konsole ausgegeben. Au√üerdem werden die Ergebnisse am Ende des Benchmarks entweder in eine .csv oder eine .json Datei mit dem Namen des Benchmarks sowie einem Zeitstempel geschrieben und unter ./results/Modell_Name abgespeichert.<br>

Wenn alle Benchmarks mithilfe von run_all_benchmarks.py ausgef√ºhrt werden, wird am Ende eine Auswertung durchgef√ºhrt. Durch diese wird eine √úbersicht der wichtigsten Ergebnisse aller durchgef√ºhrten Benchmarks in der Konsole ausgegeben, sowie die wichtigsten Ergebnisse unter ./results/benchmark_overview.csv abgespeichert.<br>

Au√üerdem werden im Rahmen der Auswertung verschiedene Plots (Balkendiagramme) zum Vergleich der evaluierten Language Models generiert und unter ./results/result_plots abgespeichert.<br>

Des Weiteren wird eine Auswertung bez√ºglich der jeweiligen Vor- und Nachteile eines Modells in Bezug auf den Vergleich zu den anderen Modellen in der Konsole ausgegeben. Diese Ergebnisse werden au√üerdem in einer Textdatei unter ./results/model_advantages.txt abgespeichert.<br>

Die Auswertung kann auch manuell √ºber den folgenden Befehl erstellt werden:
```bash
cd ./benchmarks # in den Benchmark-Ordner wechseln, wenn noch nicht getan
python benchmark_overview.py
```
Es werden dann jedoch nur die Ergebnisse dargestellt, welche auch vorhanden sind (wo bisher Benchmarks durchgef√ºhrt wurden).

---

## üí° Tipps

Manche Antwortqualit√§t-Benchmarks sind nur unter einem Linux-Betriebssystem ausf√ºhrbar und werfen unter der Ausf√ºhrung in Windows einen Fehler (z.B. HumanEval).<br>

Da das Projekt vollst√§ndig mit Python umgesetzt wurde, ist der Code betriebssystemunabh√§ngig, jedoch wurde der Code nur unter Windows 11 und WSL2-Linux (Ubuntu) getestet.<br>

Die Ausf√ºhrungszeit ist je nach Konfiguration sehr unterschiedlich, in der von mir bereitgestellten Konfiguration dauerte die vollst√§ndige Auswertung insgesamt 19 Stunden mit einer GeForce RTX 3060.<br>
Dies ist vorallem auf den Antwortqualit√§t-Benchmark HumanEval zur√ºckzuf√ºhren. Dieser dauerte alleine insgesamt rund 15 Stunden f√ºr alle drei Language Modelle.<br>

Je nach Language Modell kann es sein, das in der Konsole Fehler auftreten. Besonders bei den Antwortqualit√§t-Benchmarks kann dies aufgrund von Inkompatibilit√§t vorkommen.<br>
Getestet wurden lediglich die drei in der Konfiguration hinterlegten Modelle.<br>

Wenn ein anderes Modell getestet werden soll, muss in der Konfiguration unter "models": der Huggingface-Pfad zum Language Model angegeben werden.<br>
Dementsprechend k√∂nnen nur Language Models gebenchmarkt werden, die auch in Huggignface √∂ffentlich zug√§nglich sind.<br>

Bei der ersten Ausf√ºhrung eines Benchmarks werden die entsprechend in der Globalen Konfiguration hinterlegten Modelle von Huggingface heruntergeladen.<br>
Dies kann je nach Modellgr√∂√üe und Internetverbindung mehrere Stunden in Anspruch nehmen.<br>


