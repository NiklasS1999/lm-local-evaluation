# ğŸ§  LM-Local-Evaluation

Mithilfe dieses Repositories kÃ¶nnen Language Models wie **Qwen**, **Gemma** oder **LLaMA 3** evaluiert sowie untereinander verglichen werden.

Dazu werden je Modell folgende Kategorien gemessen, ausgewertet und grafisch dargestellt:
- InstallationsgrÃ¶ÃŸe
- Latenz
- RAM-Speichernutzung
- GPU-Speichernutzung
- ParametergrÃ¶ÃŸe
- AntwortqualitÃ¤t

Dies dient dem Vergleich von verschiedenen Language Models zur lokalen Nutzung als persÃ¶nlichen Assistenten.<br>
Dadurch kann herausgefunden werden, ob der Einsatz eines bestimmten Language Models fÃ¼r eine bestimmte Hardware-Konfiguration, beispielsweise ein mobiles EndgerÃ¤ts, sinnvoll ist.<br>
Je nach Benutzer-PrÃ¤ferenz kÃ¶nnen hierbei andere Schwerpunkte gesetzt werden und entsprechend eine Auswahl getroffen werden. Dabei soll dieses Repository unterstÃ¼tzen.<br>

Dieses Repository enthÃ¤lt eine lokal integrierte und angepasste Version von [EleutherAI/lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness) fÃ¼r die Ermittlung der AntwortqualitÃ¤t, daher muss kein separates GitHub-Repo geklont werden. Alle Ã„nderungen sind enthalten und einsatzbereit.

---

## ğŸ”§ Voraussetzungen

- Python 3.11
- Git
- Windows 11 oder Linux
- NVIDIA GPU mit CUDA 12.x (z.B. GeForce RTX 3060)
- NVIDIA CUDA-Toolkit + cuDNN-Bibliothek
- Huggingface Account & Access Token
- Virtual Environment (empfohlen)

---

## ğŸªŸ Anleitung Installation (neues Windows 11 System)

1. Python 3.11.0 herunterladen und installieren
    - Python 3.11.0 von der [offiziellen Webseite](https://www.python.org/downloads/release/python-3110/) herunterladen und installieren
    - ÃœberprÃ¼fen, ob die Installation geklappt hat:
```bash
python --version
```
2. Git herunterladen und installieren
    - Git von der [offiziellen Webseite](https://git-scm.com/downloads/win) herunterladen und installieren
    - ÃœberprÃ¼fen, ob die Installation geklappt hat:
```bash
git --version
```
3. Git Umgebungsvariable setzen
    - Windows+S Taste drÃ¼cken: â€œUmgebungsvariable bearbeitenâ€ suchen
    - WÃ¤hle â€œUmgebungsvariablenâ€
    - unter â€œSystemvariablenâ€, zu â€œPathâ€ gehen und â€œBearbeitenâ€ auswÃ¤hlen
    - â€œNeuâ€ anklicken und den Installationspfad von Git hinzufÃ¼gen (z.B. C:\Program Files\Git\bin)
    - auf â€œOKâ€ klicken und alle Fenster schlieÃŸen
    - Terminal neu starten und testen, ob git erkannt wird:
```bash
git --version
```
4. CUDA installieren
    - Ã¼berprÃ¼fen, ob die eigene [Nvidia GPU CUDA fÃ¤hig](https://developer.nvidia.com/cuda-gpus) ist
    - neueste Nvidia Grafikkarten-Softwareversion Ã¼ber z.B. Geforce Experience installieren
    - CUDA Toolkit fÃ¼r Windows [herunterladen](https://developer.nvidia.com/cuda-downloads) und installieren
    - Computer neustarten
5. cuDNN-Bibliothek installieren
    - cuDNN-Bibliothek [herunterladen](https://developer.nvidia.com/cudnn-downloads) (z.B. Windows - x86_64 - Tarball - 12)
    - heruntergeladene Dateien entpacken
    - einzelne Dateien (bin, include, lib) in die Verzeichnisse des CUDA-Installationsordners kopieren (z.B. Default: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.8)
6. CUDA Umgebungsvariable setzen
    - Windows+S Taste drÃ¼cken: â€œUmgebungsvariable bearbeitenâ€ suchen
    - WÃ¤hle â€œUmgebungsvariablenâ€
    - unter â€œSystemvariablenâ€, zu â€œPathâ€ gehen und â€œBearbeitenâ€ auswÃ¤hlen
    - â€œNeuâ€ anklicken und den Installationspfad von CUDA hinzufÃ¼gen (z.B. C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.8\bin\)
    - auf â€œOKâ€ klicken und alle Fenster schlieÃŸen
    - Terminal neu starten und testen, ob CUDA erkannt wird:
```bash
nvcc --version
```
7. lm-local-evaluation Repository klonen
    - Order erstellen, in welchem die Scripte und Modelle gespeichert werden sollen:
    ```bash
    mkdir E:\Daten\LM_Benchmarks
    ```
    
    - im Windows-Terminal in den neuen Ordner wechseln:
    ```bash
    cd /d E:\Daten\LM_Benchmarks
    ```

    - GitHub-Repository klonen:
    ```bash
    git clone https://github.com/NiklasS1999/lm-local-evaluation.git
    ```

    - Repository Ã¶ffnen:
    ```bash
    cd lm-local-evaluation
    ```
8. Virtual Environment anlegen und starten
    - Virtuelle Umgebung erstellen (z.B. mit â€œpython -m venv venvâ€)
    - Windows AusfÃ¼hrungsrechte fÃ¼r den aktuellen Benutzer setzen (z.B. Ã¼ber PowerShell als Administrator: Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser)
    - Virtuelle Umgebung starten (z.B. mit â€œvenv/Scripts/activateâ€)
9. AbhÃ¤ngigkeiten installieren
    - Pip aktualisieren mit "python.exe -m pip install --upgrade pip"
    - alle benÃ¶tigten AbhÃ¤ngigkeiten mit â€œpip install -r requirements.txtâ€œ installieren
11. Huggingface Login und Konfiguration
    - Account unter [Huggingface anlegen](https://huggingface.co/join)
    - einen Access-Token [unter den Accounteinstellungen](https://huggingface.co/settings/tokens) anlegen und kopieren
    - in der Windows Konsole â€œhuggingface-cli loginâ€ eingeben
    - Token versteckt (man sieht die Eingabe nicht!) mit einem Rechtsklick einfÃ¼gen und Enter drÃ¼cken
    - bei der Abfrage Y eingeben und Enter drÃ¼cken
    - Huggingface Model-Download Ordner setzen (z.B. Ã¼ber die Konsole mit â€œsetx HF_HOME "E:\Daten\LM_Benchmarks\models_cache"â€)


## ğŸ§ Anleitung Installation (neues Windows 11 WSL2 Linux System)

1. Im BIOS des Mainboards den SVM Mode aktivieren (damit eine Virtualisierung mÃ¶glich ist)
2. WSL2 (Ubuntu) installieren und konfigurieren
    - in der Konsole folgendes eingeben: â€œwsl --install -d Ubuntuâ€
    - in der Konsole folgendes eingeben: â€œwsl.exe --updateâ€
    - Computer neustarten
    - Windows+S Taste drÃ¼cken: â€œUbuntuâ€ suchen und Ã¶ffnen
    - Benutzernamen und Passwort setzen
    - Linux Version aktualisieren (z.B. mit â€œsudo apt update && sudo apt upgrade -yâ€)
    - Paketinstaller aktualisieren (z.B. mit â€œsudo apt updateâ€)
    - Python 3.11, Venv, Distutils, Git, Essentials, Nano installieren
        - â€œsudo apt install -y software-properties-commonâ€
        - â€œsudo add-apt-repository ppa:deadsnakes/ppaâ€
        - â€œsudo apt install -y python3.11 python3.11-venv python3.11-distutils python3.11-dev git build-essential nanoâ€
3. CUDA installieren
    - wie in der Windows Anleitung unter Punkt 4. beschrieben vorgehen (fÃ¼r WSL2 Ubuntu)
    - CUDA-Toolkit direkt unter Ubuntu installieren (z.B. mit â€œsudo apt install -y nvidia-cuda-toolkitâ€)
4. lm-local-evaluation Repository klonen
    - Order erstellen, in welchem die Scripte und Modelle gespeichert werden sollen (z.B. "mkdir LM_Benchmarks")
    - im Ubuntu-Terminal in den neuen Ordner wechseln (z.B. â€œcd ./Daten/LM_Benchmarksâ€)
    - Repository klonen mit: â€œgit clone https://github.com/NiklasS1999/lm-local-evaluation.gitâ€
    - Repository Ã¶ffnen mit: â€œcd lm-local-evaluationâ€
6. Virtual Environment anlegen und starten
    - VENV anlegen mit: â€œpython3.11 -m venv venvâ€
    - VENV aktivieren mit: â€œsource venv/bin/activateâ€
7. AbhÃ¤ngigkeiten installieren
    - Pip aktualisieren mit "python.exe -m pip install --upgrade pip"
    - alle benÃ¶tigten AbhÃ¤ngigkeiten mit â€œpip install -r requirements.txtâ€œ installieren
8. Huggingface Login und Konfiguration
    - Account unter [Huggingface anlegen](https://huggingface.co/join)
    - einen Access-Token [unter den Accounteinstellungen](https://huggingface.co/settings/tokens) anlegen und kopieren
    - in der Ubuntu Konsole â€œhuggingface-cli loginâ€ eingeben
    - Token versteckt (man sieht die Eingabe nicht!) mit einem Rechtsklick einfÃ¼gen und Enter drÃ¼cken
    - bei der Abfrage Y eingeben und Enter drÃ¼cken

---

## âš™ï¸ Konfiguration

Die Benchmarks kÃ¶nnen mithilfe von verschiedenen Einstellungen ausgefÃ¼hrt werden. Diese wirken sich drastisch auf die ErgebnisqualitÃ¤t sowie die AusfÃ¼hrungszeit aus.<br>
Die Konfiguration kann mithilfe der Datei global_config.py angepasst werden, diese ist ausfÃ¼hrlich kommentiert und zeigt mÃ¶gliche Anpassungen auf.<br>

Neben der globalen Konfiguration kann auch je Benchmark eine eigene Konfiguration gesetzt werden.<br>
Dies geschieht in den einzelnen Benchmark-Scripten.<br>

Bearbeiten der Globalen Konfiguration:
```bash
cd ./benchmarks # in den Benchmark-Ordner wechseln
nano global_config.py # nur unter Linux
```
Bearbeiten einer lokalen Konfiguration:
```bash
cd ./benchmarks # in den Benchmark-Ordner wechseln
nano model_latency.py # nur unter Linux
```

---

## ğŸš€ AusfÃ¼hrung

Es kÃ¶nnen entweder einzelne Benchmarks oder alle automatisiert nacheinander ausgefÃ¼hrt werden.

Liste an vorhandenen Benchmarks:

| Benchmark-Skript          | Beschreibung                                                                                      |
|------------------------------|-------------------------------------------------------------------------------------------------------|
| `model_installationsize.py`  | Misst die **InstallationsgrÃ¶ÃŸe** des Modells.                                                        |
| `model_latency.py`           | Misst die **Latenz**: Tokenizer-, Modell-, Antwort- und Gesamtlatenz.                                |
| `model_memoryusage.py`       | Misst die **RAM- und GPU-Speichernutzung**: Cold-Start, reine Inferenz, Gesamtverbrauch.             |
| `model_parametersize.py`     | Misst die **GrÃ¶ÃŸe der Modellparameter** â€“ als Anhaltspunkt fÃ¼r den geschÃ¤tzten Rechenaufwand.        |
| `model_responsequality.py`   | Bewertet die **AntwortqualitÃ¤t** anhand vordefinierter Benchmarks (z.â€¯B. MMLU, HellaSwag, GSM8K).    |

AusfÃ¼hrung eines einzelnen Benchmarks:
```bash
cd ./benchmarks # in den Benchmark-Ordner wechseln
python model_installationsize.py
```
AusfÃ¼hrung aller Benchmarks inkl. Auswertung:
```bash
cd ./benchmarks # in den Benchmark-Ordner wechseln
python run_all_benchmarks.py
```

---

## ğŸ“Š Ergebnisse

WÃ¤hrend der AusfÃ¼hrung eines Benchmarks werden die Ergebnisse in der Konsole ausgegeben. AuÃŸerdem werden die Ergebnisse am Ende des Benchmarks entweder in eine .csv oder eine .json Datei mit dem Namen des Benchmarks sowie einem Zeitstempel geschrieben und unter ./results/Modell_Name abgespeichert.<br>

Wenn alle Benchmarks mithilfe von run_all_benchmarks.py ausgefÃ¼hrt werden, wird am Ende eine Auswertung durchgefÃ¼hrt. Durch diese wird eine Ãœbersicht der wichtigsten Ergebnisse aller durchgefÃ¼hrten Benchmarks in der Konsole ausgegeben, sowie die wichtigsten Ergebnisse unter ./results/benchmark_overview.csv abgespeichert.<br>

AuÃŸerdem werden im Rahmen der Auswertung verschiedene Plots (Balkendiagramme) zum Vergleich der evaluierten Language Models generiert und unter ./results/result_plots abgespeichert.<br>

Des Weiteren wird eine Auswertung bezÃ¼glich der jeweiligen Vor- und Nachteile eines Modells in Bezug auf den Vergleich zu den anderen Modellen in der Konsole ausgegeben. Diese Ergebnisse werden auÃŸerdem in einer Textdatei unter ./results/model_advantages.txt abgespeichert.<br>

Die Auswertung kann auch manuell Ã¼ber den folgenden Befehl erstellt werden:
```bash
python benchmark_overview.py
```
Es werden dann jedoch nur die Ergebnisse dargestellt, welche auch vorhanden sind (wo bisher Benchmarks durchgefÃ¼hrt wurden).

---

## ğŸ’¡ Tipps

Manche AntwortqualitÃ¤t-Benchmarks sind nur unter einem Linux-Betriebssystem ausfÃ¼hrbar und werfen unter der AusfÃ¼hrung in Windows einen Fehler (z.B. HumanEval).<br>

Da das Projekt vollstÃ¤ndig mit Python umgesetzt wurde, ist der Code betriebssystemunabhÃ¤ngig, jedoch wurde der Code nur unter Windows 11 und WSL2-Linux (Ubuntu) getestet.<br>

Die AusfÃ¼hrungszeit ist je nach Konfiguration sehr unterscheidlich, in der von mir bereitgestellten Konfiguration dauerte die vollstÃ¤ndige Auswertung insgesamt 19 Stunden mit einer GeForce RTX 3060.<br>
Dies ist vorallem auf den AntwortqualitÃ¤t-Benchmark HumanEval zurÃ¼ckzufÃ¼hren. Dieser dauerte alleine rund 15 Stunden fÃ¼r alle drei Language Modelle.<br>

Je nach Language Modell kann es sein, das in der Konsole Fehler auftreten. Besonders bei den AntwortqualitÃ¤t-Benchmarks kann dies aufgrund von InkompatibilitÃ¤t vorkommen.<br>
Getestet wurden lediglich die drei in der Konfiguration hinterlegten Modelle.<br>

Wenn ein anderes Modell getestet werden soll, muss in der Konfiguration unter "models": der Huggingface-Pfad zum Language Model angegeben werden.<br>
Dementsprechend kÃ¶nnen nur Language Models gebenchmarkt werden, die auch in Huggignface Ã¶ffentlich zugÃ¤nglich sind.<br>

Bei der ersten AusfÃ¼hrung eines Benchmarks werden die entsprechend in der Globalen konfiguration hinterlegten Modelle von Huggingface heruntergeladen.<br>
Dies kann je nach ModellgrÃ¶ÃŸe und Internetverbindung mehrere Stunden in Anspruch nehmen.<br>


